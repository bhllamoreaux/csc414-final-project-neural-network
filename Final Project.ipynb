{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project - Artificial Neural Networks for Image Classification\n",
    "For this project, I decided to try to implement my very own neural network from basically scratch. I consulted some online references to help, obviously, and I think it turned out pretty well. It took a long time, and was more time consuming than expected, but I got something to work at the end.\n",
    "\n",
    "There are improvements that I'd like to make, but unfortunately I ran out of time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First, we need to import all required libraries and define some utility functions\n",
    "\n",
    "I'm using the same image set from Project 3 for this one too, hence why all of the object names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "from sklearn.utils import shuffle\n",
    "from skimage import io\n",
    "from skimage import img_as_float32\n",
    "import time\n",
    "import pickle\n",
    "from IPython.display import display, clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "objectNames = ['Cup','Boat','Cabinet','Car','Chair','Flashlight','Handle',\n",
    "                      'HoseReel','Keyboard', 'LED','Light1','Light2','Mug','Scooter',\n",
    "                      'SprayBottle','Stapler','Trash','ibook01','imac04','imac98']\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "#derivative of the sigmoid function\n",
    "def sigmoid_prime(z):\n",
    "    return sigmoid(z)*(1-sigmoid(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define our neural network class\n",
    "Neural networks are quite complicated, so I'll break it down by it's constituent parts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First we have the init function. \n",
    "\n",
    "This one is pretty obvious, all it does is create the initial weights and biases for the Neural Network, based on the size array. For example, a [2,3,1] size array would have 2 input nodes, 3 hidden nodes and 1 output node.\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __init__(self, sizes):\n",
    "    #first lets set the biases for each node to random number between 0 and 1\n",
    "    self.biases = [np.random.randn(y,1) for y in sizes[1:]]\n",
    "    #then set the weights for each link to random number between 0 and 1\n",
    "    self.weights = [np.random.randn(y,x) for x, y in zip(sizes[:-1], sizes[1:])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next we have the feed forward function. \n",
    "This just defines how the input signal gets propagated through the network to produce a result. Basically, the value at every neuron is the product of the signals of the last layer, the weights on the links between the last layer and that neuron, and the bias of that particular neuron.\n",
    "\n",
    "Therefore, to calculate the output result, we can simply calculate the result at every neuron and just move through the layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to calculate the result for any given node, just multiply the input values by the weights of the links, then add the bias\n",
    "#loop through every node\n",
    "def feedForward(self,inputVals):\n",
    "    i = 0\n",
    "    #select node\n",
    "    for bias,weight in zip(self.biases,self.weights):\n",
    "        #create a new layer to store results in\n",
    "        layer = np.zeros(np.shape(bias))\n",
    "        #select input weights and bias for that node\n",
    "        for b, w in zip(bias,weight):\n",
    "            #calculate the value for a specific node with the biases and weights.\n",
    "            #sigmoid to keep it within 0 and 1\n",
    "            layer[i] = sigmoid(np.dot(w, inputVals) + b)\n",
    "            i+=1\n",
    "\n",
    "         #the last layer is now the inputs for the new layer\n",
    "        inputVals = layer\n",
    "\n",
    "        i=0\n",
    "    return inputVals\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thirdly, we have a very important function - the backpropagation function. \n",
    "\n",
    "This function defines how the weights are biases are updated throughout the network in order to get us closer to the correct result. Backpropagation works by computing the error of the output, then propagating the error backwards through the neural network in order to update the weights and biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "#back propagate with gradient descent through the neural network to adjust weights and biases\n",
    "def backPropagation(self,inputVals,expectedVals):\n",
    "    #hold the new biases and weights\n",
    "    new_biases = [np.zeros(b.shape) for b in self.biases]\n",
    "    new_weights = [np.zeros(w.shape) for w in self.weights]\n",
    "    #create a new variable to hold input values\n",
    "    inputs = np.expand_dims(inputVals,axis=1)\n",
    "    #list to hold values of every layer\n",
    "    activations = []\n",
    "    activations.append(inputVals)\n",
    "    #list to hold the activation values after the activation function has been applied to every layer\n",
    "    outputs = []\n",
    "    i = 0\n",
    "    #feed forward\n",
    "    for bias,weight in zip(self.biases,self.weights):\n",
    "        #create a new layer to store results in\n",
    "        activationLayer = np.zeros(np.shape(bias))\n",
    "        outputLayer = np.zeros(np.shape(bias))\n",
    "        #select input weights and bias for that node\n",
    "        for b, w in zip(bias,weight):\n",
    "            #calculate the value for a specific node with the biases and weights.\n",
    "            #sigmoid to keep it within 0 and 1\n",
    "            activationLayer[i] = sigmoid(np.dot(w, inputVals) + b)\n",
    "            outputLayer[i] = np.dot(w, inputVals) + b\n",
    "            i+=1\n",
    "\n",
    "         #the last layer is now the inputs for the new layer\n",
    "        inputVals = activationLayer\n",
    "        #add the activation layer to the list\n",
    "        activations.append(activationLayer)\n",
    "        outputs.append(outputLayer)\n",
    "        i = 0\n",
    "\n",
    "    #print(self.weights[0].shape,self.weights[1].shape)\n",
    "    #get a vector of errors for the last layer - output layer\n",
    "    if np.shape(expectedVals) != ():\n",
    "        outputError = (activationLayer-np.expand_dims(expectedVals,axis=1)) * sigmoid_prime(outputLayer)\n",
    "    else:\n",
    "        outputError = (activationLayer-expectedVals) * sigmoid_prime(outputLayer)\n",
    "\n",
    "    #set the new weights for the last layer\n",
    "    new_weights[-1] = np.dot(outputError,activations[-2].T)\n",
    "    new_biases[-1] = outputError\n",
    "\n",
    "     #make sure the shape is correct\n",
    "    if len(activations[0].shape) == 1:\n",
    "        activations[0] = np.expand_dims(activations[0],axis=1)\n",
    "\n",
    "    i = 2\n",
    "    while i < len(activations):\n",
    "        outputError = np.dot(self.weights[-i+1].T,outputError) * sigmoid_prime(outputs[-i])\n",
    "        new_biases[-i] = outputError\n",
    "        new_weights[-i] = np.dot(outputError,activations[-i-1].T)\n",
    "        i+=1\n",
    "\n",
    "    return new_biases, new_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we need a way for our network to learn. That's where the gradient descent comes in. \n",
    "Gradient descent is the process of gradually updating the weights and biases of the network in the correct direction until we reach a local minimum of the cost function.\n",
    "\n",
    "In simple terms, all it does is move the network in the right direction every time it's called"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pass in the lists of weights and biases for each layer, as well as the training data and the correct values\n",
    "#that can be expected for the training data\n",
    "def gradientDescent(self,training_vals, correct_vals,learning_constant):\n",
    "    #create a new list to hold the sums of all of the weight corrections\n",
    "    weights_sums = copy.deepcopy(self.weights)\n",
    "    for w in weights_sums:\n",
    "        w[:] = 0\n",
    "\n",
    "    #create a new list to hold the sums of all of the bias corrections\n",
    "    biases_sums = copy.deepcopy(self.biases)\n",
    "    for b in biases_sums:\n",
    "        b[:] = 0\n",
    "\n",
    "    #run for every training instance\n",
    "    for instance,correction in zip(training_vals,correct_vals):\n",
    "        #convert list to numpy array\n",
    "        instance = np.asarray(instance)\n",
    "        #get the error and activation vectors back\n",
    "        b,w = self.backPropagation(instance,correction)\n",
    "        #calculate the sum for the weights and biases\n",
    "        instance = np.asarray([instance])\n",
    "       # print(weights_sums[0].shape)\n",
    "       # print(w[0])\n",
    "        i = 0\n",
    "        while i < len(b) - 1:\n",
    "            if i > 0:\n",
    "                weights_sums[i] += w[i]\n",
    "            else:\n",
    "                weights_sums[i] += w[i]\n",
    "\n",
    "            biases_sums[i] += b[i]\n",
    "\n",
    "            i+=1\n",
    "\n",
    "    new_weights = list()\n",
    "    new_biases = list()\n",
    "    #go through the network and update all of the weights and biases based on the values we calculated\n",
    "    #previously\n",
    "    i = 0\n",
    "    while i < len(self.weights):\n",
    "        w = self.weights[i] - learning_constant * weights_sums[i]\n",
    "        b = self.biases[i] - learning_constant * biases_sums[i]\n",
    "        new_weights.append(w)\n",
    "        new_biases.append(b)\n",
    "        i+=1\n",
    "\n",
    "    #update the weights and biases for the model\n",
    "    self.weights = new_weights\n",
    "    self.biases = new_biases\n",
    "    return new_weights, new_biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And last but not least, we have our stochastic gradient descent function. \n",
    "\n",
    "While regular gradient descent requires training on every training object before updating the results, stochastic gradient descent allows us to train more quickly, by learning from random samples of training data instead of all of it at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is how the network learns on a large scale set of data\n",
    "#where the batches are how many times the data should be randomly sampled\n",
    "#and epochs is how many times we want it to learn. Can be less than batches\n",
    "def stochasticGradientDescent(self, trainingData, expectedData, numBatches, epochs, learningRate):\n",
    "    numInBatch = int(len(trainingData) / numBatches)\n",
    "\n",
    "    #train the network\n",
    "    i = 0\n",
    "    while i < epochs:\n",
    "        #shuffle the data randomly\n",
    "        trainingData, expectedData = shuffle(trainingData,expectedData)\n",
    "        self.gradientDescent(trainingData[0:numInBatch],expectedData[0:numInBatch],learningRate)\n",
    "        clear_output(wait=True)\n",
    "        display('epoch #'+str(i)+' done')\n",
    "        i+=1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When we put it all together, we get this beautiful neural network class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(object):\n",
    "\n",
    "    def __init__(self, sizes):\n",
    "        #first lets set the biases for each node to random number between 0 and 1\n",
    "        self.biases = [np.random.randn(y,1) for y in sizes[1:]]\n",
    "        #then set the weights for each link to random number between 0 and 1\n",
    "        self.weights = [np.random.randn(y,x) for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "        \n",
    "        \n",
    "    #to calculate the result for any given node, just multiply the input values by the weights of the links, then add the bias\n",
    "    #loop through every node\n",
    "    def feedForward(self,inputVals):\n",
    "        i = 0\n",
    "        #select node\n",
    "        for bias,weight in zip(self.biases,self.weights):\n",
    "            #create a new layer to store results in\n",
    "            layer = np.zeros(np.shape(bias))\n",
    "            #select input weights and bias for that node\n",
    "            for b, w in zip(bias,weight):\n",
    "                #calculate the value for a specific node with the biases and weights.\n",
    "                #sigmoid to keep it within 0 and 1\n",
    "                layer[i] = sigmoid(np.dot(w, inputVals) + b)\n",
    "                i+=1\n",
    "\n",
    "             #the last layer is now the inputs for the new layer\n",
    "            inputVals = layer\n",
    "\n",
    "            i=0\n",
    "        return inputVals\n",
    "\n",
    "    #back propagate with gradient descent through the neural network to adjust weights and biases\n",
    "    def backPropagation(self,inputVals,expectedVals):\n",
    "        #hold the new biases and weights\n",
    "        new_biases = [np.zeros(b.shape) for b in self.biases]\n",
    "        new_weights = [np.zeros(w.shape) for w in self.weights]\n",
    "        #create a new variable to hold input values\n",
    "        inputs = np.expand_dims(inputVals,axis=1)\n",
    "        #list to hold values of every layer\n",
    "        activations = []\n",
    "        activations.append(inputVals)\n",
    "        #list to hold the activation values after the activation function has been applied to every layer\n",
    "        outputs = []\n",
    "        i = 0\n",
    "        #feed forward\n",
    "        for bias,weight in zip(self.biases,self.weights):\n",
    "            #create a new layer to store results in\n",
    "            activationLayer = np.zeros(np.shape(bias))\n",
    "            outputLayer = np.zeros(np.shape(bias))\n",
    "            #select input weights and bias for that node\n",
    "            for b, w in zip(bias,weight):\n",
    "                #calculate the value for a specific node with the biases and weights.\n",
    "                #sigmoid to keep it within 0 and 1\n",
    "                activationLayer[i] = sigmoid(np.dot(w, inputVals) + b)\n",
    "                outputLayer[i] = np.dot(w, inputVals) + b\n",
    "                i+=1\n",
    "\n",
    "             #the last layer is now the inputs for the new layer\n",
    "            inputVals = activationLayer\n",
    "            #add the activation layer to the list\n",
    "            activations.append(activationLayer)\n",
    "            outputs.append(outputLayer)\n",
    "            i = 0\n",
    "\n",
    "        #print(self.weights[0].shape,self.weights[1].shape)\n",
    "        #get a vector of errors for the last layer - output layer\n",
    "        if np.shape(expectedVals) != ():\n",
    "            outputError = (activationLayer-np.expand_dims(expectedVals,axis=1)) * sigmoid_prime(outputLayer)\n",
    "        else:\n",
    "            outputError = (activationLayer-expectedVals) * sigmoid_prime(outputLayer)\n",
    "\n",
    "        #set the new weights for the last layer\n",
    "        new_weights[-1] = np.dot(outputError,activations[-2].T)\n",
    "        new_biases[-1] = outputError\n",
    "        \n",
    "         #make sure the shape is correct\n",
    "        if len(activations[0].shape) == 1:\n",
    "            activations[0] = np.expand_dims(activations[0],axis=1)\n",
    "            \n",
    "        i = 2\n",
    "        while i < len(activations):\n",
    "            outputError = np.dot(self.weights[-i+1].T,outputError) * sigmoid_prime(outputs[-i])\n",
    "            new_biases[-i] = outputError\n",
    "            new_weights[-i] = np.dot(outputError,activations[-i-1].T)\n",
    "            i+=1\n",
    "        \n",
    "        return new_biases, new_weights\n",
    "\n",
    "    #pass in the lists of weights and biases for each layer, as well as the training data and the correct values\n",
    "    #that can be expected for the training data\n",
    "    def gradientDescent(self,training_vals, correct_vals,learning_constant):\n",
    "        #create a new list to hold the sums of all of the weight corrections\n",
    "        weights_sums = copy.deepcopy(self.weights)\n",
    "        for w in weights_sums:\n",
    "            w[:] = 0\n",
    "\n",
    "        #create a new list to hold the sums of all of the bias corrections\n",
    "        biases_sums = copy.deepcopy(self.biases)\n",
    "        for b in biases_sums:\n",
    "            b[:] = 0\n",
    "        \n",
    "        #run for every training instance\n",
    "        for instance,correction in zip(training_vals,correct_vals):\n",
    "            #convert list to numpy array\n",
    "            instance = np.asarray(instance)\n",
    "            #get the error and activation vectors back\n",
    "            b,w = self.backPropagation(instance,correction)\n",
    "            #calculate the sum for the weights and biases\n",
    "            instance = np.asarray([instance])\n",
    "           # print(weights_sums[0].shape)\n",
    "           # print(w[0])\n",
    "            i = 0\n",
    "            while i < len(b) - 1:\n",
    "                if i > 0:\n",
    "                    weights_sums[i] += w[i]\n",
    "                else:\n",
    "                    weights_sums[i] += w[i]\n",
    "\n",
    "                biases_sums[i] += b[i]\n",
    "\n",
    "                i+=1\n",
    "\n",
    "        new_weights = list()\n",
    "        new_biases = list()\n",
    "        #go through the network and update all of the weights and biases based on the values we calculated\n",
    "        #previously\n",
    "        i = 0\n",
    "        while i < len(self.weights):\n",
    "            w = self.weights[i] - learning_constant * weights_sums[i]\n",
    "            b = self.biases[i] - learning_constant * biases_sums[i]\n",
    "            new_weights.append(w)\n",
    "            new_biases.append(b)\n",
    "            i+=1\n",
    "\n",
    "        #update the weights and biases for the model\n",
    "        self.weights = new_weights\n",
    "        self.biases = new_biases\n",
    "        return new_weights, new_biases\n",
    "    \n",
    "    #this is how the network learns on a large scale set of data\n",
    "    #where the batches are how many times the data should be randomly sampled\n",
    "    #and epochs is how many times we want it to learn. Can be less than batches\n",
    "    def stochasticGradientDescent(self, trainingData, expectedData, numBatches, epochs, learningRate):\n",
    "        numInBatch = int(len(trainingData) / numBatches)\n",
    "        \n",
    "        #train the network\n",
    "        i = 0\n",
    "        while i < epochs:\n",
    "            #shuffle the data randomly\n",
    "            trainingData, expectedData = shuffle(trainingData,expectedData)\n",
    "            self.gradientDescent(trainingData[0:numInBatch],expectedData[0:numInBatch],learningRate)\n",
    "            clear_output(wait=True)\n",
    "            display('epoch #'+str(i)+' done')\n",
    "            i+=1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Just to prove that this neural network works, here is a simple example with the xor pattern. As expected, it predicts high values for (0,1) and (1,0), but low values for (0,0) and (1,1)\n",
    "\n",
    "If it doesn't work the first time, just run it again. Sometimes it can be weird with the initialized values, but it should work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'epoch #4999 done'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.1871733]]\n",
      "[[0.65075546]]\n",
      "[[0.83473509]]\n",
      "[[0.23649608]]\n"
     ]
    }
   ],
   "source": [
    "#xor pattern\n",
    "inputData = [[0,0],[0,1],[1,0],[1,1]]\n",
    "\n",
    "outputData = [0,1,1,0]\n",
    "\n",
    "#build a simple network with two input nodes, 10 hidden nodes and one output node.\n",
    "sizes = [2,10,1]\n",
    "\n",
    "network = NeuralNetwork(sizes)\n",
    "\n",
    "#train the network\n",
    "network.stochasticGradientDescent(inputData,outputData,1,5000,0.9)\n",
    "\n",
    "for pt in inputData:\n",
    "    print(network.feedForward(pt))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## These are just some functions I ported over from Project 3 to process in the training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def BuildImageMatrix(folderName):\n",
    "    \n",
    "    #allocate space for the image\n",
    "    image = np.zeros(16384,dtype=float)\n",
    "    \n",
    "    i = 0\n",
    "    while i < 128:\n",
    "        filename = folderName + 'img_' + str(i) + '.png'\n",
    "        #read image in\n",
    "        newimg = io.imread(filename)\n",
    "        #normalize the values to between 1 and 0\n",
    "        newimg = img_as_float32(newimg)\n",
    "        #flatten image into a vector\n",
    "        newimg = newimg.flatten('F')\n",
    "        if i > 0:\n",
    "            image = np.vstack((image,newimg))\n",
    "        else:\n",
    "            image = newimg\n",
    "        i+=1\n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in all of our training data\n",
    "#build the label data\n",
    "labels = []\n",
    "matrices = []\n",
    "i=0\n",
    "for obj in objectNames:\n",
    "        matrices.append(BuildImageMatrix('P4Images/TrainingImages/'+obj+'64/UnProcessed/'))\n",
    "        if i>0:\n",
    "            imageMatrix = np.vstack((imageMatrix,matrices[i]))\n",
    "        else:\n",
    "            imageMatrix = matrices[i]\n",
    "        \n",
    "        expectedValue = np.zeros(20)\n",
    "        expectedValue[i] = 1\n",
    "        labels.append(expectedValue)\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "expectedVals = labels[0]\n",
    "j=0\n",
    "for obj in objectNames:\n",
    "    i=0\n",
    "    while i < 128:\n",
    "        expectedVals = np.vstack((expectedVals,labels[j]))\n",
    "        i+=1\n",
    "    i=0\n",
    "    j+=1\n",
    "    \n",
    "expectedVals = np.delete(expectedVals,1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ChooseTestObject():\n",
    "    print('List of Test Objects:')\n",
    "    for obj in objectNames:\n",
    "        print(obj)\n",
    "        \n",
    "    print('\\n\\nEnter in the object name: ')\n",
    "    \n",
    "    obj = input()\n",
    "    \n",
    "    print('Enter in the test image number: ')\n",
    "    \n",
    "    imageNumber = input()\n",
    "    \n",
    "    img = io.imread('P4Images/TestImages/'+obj+'32/UnProcessed/img_'+imageNumber+'.png')\n",
    "    \n",
    "    return img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obviously it would take too long for you to sit there and train your own model, so I'll just load a model that I spent some time pretraining already.\n",
    "\n",
    "I used the pickle library to just save it in a file so I can load it and mess with it any time I want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuralNet = pickle.load( open( \"model.p\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's run some tests and see what kind of accuracy we can get. Note that running through all 1280 test images, which the model has never seen, only takes a few seconds.\n",
    "\n",
    "This model has 16384 input neurons, a hidden layer of 650 neurons and a final layer of 20 neurons, to represent each of the 20 classes. \n",
    "\n",
    "I initally trained it by running a gradient descent on one image class, but after I got the weights initialized, it was easy to just pass it into the stochastic gradient descent function and let it go. Unfortunately, due to time constraints, it only got up to about 65% accuracy. If I had more time, I'm sure we'd see it get up into the 80s and 90s.\n",
    "\n",
    "Ideally, I'd implement a convolutional layer on top of this for even better accuracy and performace, and then even an R layer on top of that, to make it a state of the art R-CNN. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cup: Got 64 correct out of 64, or 100%\n",
      "Boat: Got 64 correct out of 64, or 100%\n",
      "Cabinet: Got 0 correct out of 64, or 0%\n",
      "Car: Got 64 correct out of 64, or 100%\n",
      "Chair: Got 64 correct out of 64, or 100%\n",
      "Flashlight: Got 0 correct out of 64, or 0%\n",
      "Handle: Got 64 correct out of 64, or 100%\n",
      "HoseReel: Got 0 correct out of 64, or 0%\n",
      "Keyboard: Got 64 correct out of 64, or 100%\n",
      "LED: Got 64 correct out of 64, or 100%\n",
      "Light1: Got 64 correct out of 64, or 100%\n",
      "Light2: Got 64 correct out of 64, or 100%\n",
      "Mug: Got 0 correct out of 64, or 0%\n",
      "Scooter: Got 64 correct out of 64, or 100%\n",
      "SprayBottle: Got 0 correct out of 64, or 0%\n",
      "Stapler: Got 64 correct out of 64, or 100%\n",
      "Trash: Got 0 correct out of 64, or 0%\n",
      "ibook01: Got 64 correct out of 64, or 100%\n",
      "imac04: Got 0 correct out of 64, or 0%\n",
      "imac98: Got 64 correct out of 64, or 100%\n",
      "Got 832 correct out of 1280, or 65%\n"
     ]
    }
   ],
   "source": [
    "testingVals = labels[0]\n",
    "j=0\n",
    "for obj in objectNames:\n",
    "    i=0\n",
    "    while i < 64:\n",
    "        testingVals = np.vstack((testingVals,labels[j]))\n",
    "        i+=1\n",
    "    i=0\n",
    "    j+=1\n",
    "    \n",
    "testingVals = np.delete(testingVals,1,0)\n",
    "\n",
    "matches = 0\n",
    "overall = 0\n",
    "j=0\n",
    "for obj in objectNames:\n",
    "    i=0\n",
    "    while i < 64:\n",
    "        testImg = io.imread('P4Images/TestImages/'+obj+'32/UnProcessed/img_'+str(i)+'.png')\n",
    "        testImg = img_as_float32(testImg)\n",
    "        result = neuralNet.feedForward(testImg.flatten('F'))\n",
    "        if np.argmax(result) == np.argmax(testingVals[j]):\n",
    "            matches+=1\n",
    "            overall+=1\n",
    "        i+=1\n",
    "        j+=1\n",
    "    print(obj+': Got '+ str(matches) + ' correct out of 64, or ' + str(int((matches/64)*100))+'%')\n",
    "    matches = 0\n",
    "        \n",
    "        \n",
    "print('Got '+ str(overall) + ' correct out of 1280, or ' +str(int((overall/1280)*100))+'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To illustrate further, here is a little user interactive window. You can pick the test image and it will tell you the predicted class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of Test Objects:\n",
      "Cup\n",
      "Boat\n",
      "Cabinet\n",
      "Car\n",
      "Chair\n",
      "Flashlight\n",
      "Handle\n",
      "HoseReel\n",
      "Keyboard\n",
      "LED\n",
      "Light1\n",
      "Light2\n",
      "Mug\n",
      "Scooter\n",
      "SprayBottle\n",
      "Stapler\n",
      "Trash\n",
      "ibook01\n",
      "imac04\n",
      "imac98\n",
      "\n",
      "\n",
      "Enter in the object name: \n",
      "Mug\n",
      "Enter in the test image number: \n",
      "12\n",
      "Predicted Class - Cup\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img = ChooseTestObject()\n",
    "plt.imshow(img,cmap='gray')\n",
    "plt.axis('off')\n",
    "#normalize the values to between 1 and 0\n",
    "img = img_as_float32(img)\n",
    "#flatten image into a vector\n",
    "img = img.flatten('F')\n",
    "\n",
    "result = neuralNet.feedForward(img)\n",
    "print('Predicted Class - '+objectNames[np.argmax(result)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'epoch #1999 done'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cup: Got 64 correct out of 64, or 100%\n",
      "Boat: Got 64 correct out of 64, or 100%\n",
      "Cabinet: Got 0 correct out of 64, or 0%\n",
      "Car: Got 64 correct out of 64, or 100%\n",
      "Chair: Got 64 correct out of 64, or 100%\n",
      "Flashlight: Got 0 correct out of 64, or 0%\n",
      "Handle: Got 64 correct out of 64, or 100%\n",
      "HoseReel: Got 1 correct out of 64, or 1%\n",
      "Keyboard: Got 64 correct out of 64, or 100%\n",
      "LED: Got 64 correct out of 64, or 100%\n",
      "Light1: Got 64 correct out of 64, or 100%\n",
      "Light2: Got 64 correct out of 64, or 100%\n",
      "Mug: Got 0 correct out of 64, or 0%\n",
      "Scooter: Got 64 correct out of 64, or 100%\n",
      "SprayBottle: Got 0 correct out of 64, or 0%\n",
      "Stapler: Got 64 correct out of 64, or 100%\n",
      "Trash: Got 0 correct out of 64, or 0%\n",
      "ibook01: Got 64 correct out of 64, or 100%\n",
      "imac04: Got 0 correct out of 64, or 0%\n",
      "imac98: Got 64 correct out of 64, or 100%\n",
      "Got 833 correct out of 1280, or 65%\n"
     ]
    }
   ],
   "source": [
    "#load the saved model that we ran before\n",
    "neuralNet = pickle.load( open( \"model.p\", \"rb\" ) )\n",
    "neuralNet.stochasticGradientDescent(imageMatrix,expectedVals,20,2000,0.9)\n",
    "pickle.dump( neuralNet, open( \"model.p\", \"wb\" ) )\n",
    "\n",
    "testingVals = labels[0]\n",
    "j=0\n",
    "for obj in objectNames:\n",
    "    i=0\n",
    "    while i < 64:\n",
    "        testingVals = np.vstack((testingVals,labels[j]))\n",
    "        i+=1\n",
    "    i=0\n",
    "    j+=1\n",
    "    \n",
    "testingVals = np.delete(testingVals,1,0)\n",
    "\n",
    "matches = 0\n",
    "overall = 0\n",
    "j=0\n",
    "for obj in objectNames:\n",
    "    i=0\n",
    "    while i < 64:\n",
    "        testImg = io.imread('P4Images/TestImages/'+obj+'32/UnProcessed/img_'+str(i)+'.png')\n",
    "        testImg = img_as_float32(testImg)\n",
    "        result = neuralNet.feedForward(testImg.flatten('F'))\n",
    "        if np.argmax(result) == np.argmax(testingVals[j]):\n",
    "            matches+=1\n",
    "            overall+=1\n",
    "        i+=1\n",
    "        j+=1\n",
    "    print(obj+': Got '+ str(matches) + ' correct out of 64, or ' + str(int((matches/64)*100))+'%')\n",
    "    matches = 0\n",
    "        \n",
    "        \n",
    "print('Got '+ str(overall) + ' correct out of 1280, or ' +str(int((overall/1280)*100))+'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
